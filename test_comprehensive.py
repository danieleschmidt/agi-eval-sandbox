#!/usr/bin/env python3
"""
Comprehensive test suite for AGI Evaluation Sandbox.
Tests core functionality without external dependencies.
"""

import asyncio
import sys
import os
from pathlib import Path

# Add the project root to Python path
sys.path.insert(0, str(Path(__file__).parent))

def test_imports():
    """Test that all core modules can be imported."""
    print("üß™ Testing imports...")
    
    try:
        from src.agi_eval_sandbox.core import EvalSuite, Model
        print("‚úÖ Core modules imported successfully")
    except Exception as e:
        print(f"‚ùå Core import failed: {e}")
        return False
    
    try:
        from src.agi_eval_sandbox.core.models import create_mock_model, create_openai_model, create_anthropic_model
        print("‚úÖ Model factory functions imported successfully")
    except Exception as e:
        print(f"‚ùå Model factory import failed: {e}")
        return False
    
    try:
        from src.agi_eval_sandbox.core.benchmarks import TruthfulQABenchmark, MMLUBenchmark, HumanEvalBenchmark, CustomBenchmark
        print("‚úÖ Benchmark classes imported successfully")
    except Exception as e:
        print(f"‚ùå Benchmark import failed: {e}")
        return False
    
    try:
        from src.agi_eval_sandbox.core.validation import InputValidator, ResourceValidator
        print("‚úÖ Validation classes imported successfully")
    except Exception as e:
        print(f"‚ùå Validation import failed: {e}")
        return False
    
    try:
        from src.agi_eval_sandbox.core.health import health_monitor
        print("‚úÖ Health monitoring imported successfully")
    except Exception as e:
        print(f"‚ùå Health monitoring import failed: {e}")
        return False
    
    return True

def test_model_creation():
    """Test model creation functionality."""
    print("\nüß™ Testing model creation...")
    
    try:
        from src.agi_eval_sandbox.core.models import create_mock_model
        
        # Test basic mock model
        model = create_mock_model("test-model")
        print("‚úÖ Basic mock model created")
        
        # Test mock model with custom parameters
        model = create_mock_model(
            "advanced-test-model",
            simulate_delay=0.001,
            simulate_failures=False,
            response_template="Test response from {model}: {prompt}"
        )
        print("‚úÖ Advanced mock model created")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Model creation failed: {e}")
        return False

def test_benchmarks():
    """Test benchmark functionality."""
    print("\nüß™ Testing benchmarks...")
    
    try:
        from src.agi_eval_sandbox.core.benchmarks import TruthfulQABenchmark, MMLUBenchmark, HumanEvalBenchmark
        
        # Test TruthfulQA
        truthful_qa = TruthfulQABenchmark()
        questions = truthful_qa.get_questions()
        print(f"‚úÖ TruthfulQA benchmark: {len(questions)} questions loaded")
        
        # Test MMLU
        mmlu = MMLUBenchmark()
        questions = mmlu.get_questions()
        print(f"‚úÖ MMLU benchmark: {len(questions)} questions loaded")
        
        # Test HumanEval
        humaneval = HumanEvalBenchmark()
        questions = humaneval.get_questions()
        print(f"‚úÖ HumanEval benchmark: {len(questions)} questions loaded")
        
        # Test question evaluation
        question = questions[0]
        score = humaneval.evaluate_response(question, "def test(): return True")
        print(f"‚úÖ Question evaluation: score = {score.value}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Benchmark testing failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_model_generation():
    """Test model text generation."""
    print("\nüß™ Testing model generation...")
    
    try:
        from src.agi_eval_sandbox.core.models import create_mock_model
        
        model = create_mock_model("test-model", simulate_delay=0.001)
        
        # Test single generation
        response = await model.generate("Hello, world!")
        print(f"‚úÖ Single generation: {response[:50]}...")
        
        # Test batch generation
        prompts = ["Hello", "How are you?", "Goodbye"]
        responses = await model.batch_generate(prompts)
        print(f"‚úÖ Batch generation: {len(responses)} responses generated")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Model generation failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_evaluation_suite():
    """Test the main evaluation suite."""
    print("\nüß™ Testing evaluation suite...")
    
    try:
        from src.agi_eval_sandbox.core import EvalSuite
        from src.agi_eval_sandbox.core.models import create_mock_model
        
        # Create suite and model
        suite = EvalSuite(max_concurrent_evaluations=2)
        model = create_mock_model("test-model", simulate_delay=0.001)
        
        print("‚úÖ Suite and model created")
        
        # Test listing benchmarks
        benchmarks = suite.list_benchmarks()
        print(f"‚úÖ Found {len(benchmarks)} benchmarks: {', '.join(benchmarks)}")
        
        # Test single benchmark evaluation
        results = await suite.evaluate(
            model=model,
            benchmarks=['truthfulqa'],
            num_questions=2,
            save_results=False,
            parallel=False
        )
        
        summary = results.summary()
        print(f"‚úÖ Single benchmark evaluation completed: {summary['total_questions']} questions")
        
        # Test multiple benchmark evaluation
        results = await suite.evaluate(
            model=model,
            benchmarks=['truthfulqa', 'mmlu'],
            num_questions=1,
            save_results=False,
            parallel=True
        )
        
        summary = results.summary()
        print(f"‚úÖ Multiple benchmark evaluation completed: {summary['total_benchmarks']} benchmarks")
        
        # Test optimized evaluation
        try:
            results = await suite.evaluate_optimized(
                model=model,
                benchmarks=['truthfulqa'],
                num_questions=1,
                optimization_level="speed",
                use_cache=True,
                parallel=True
            )
            summary = results.summary()
            print(f"‚úÖ Optimized evaluation completed: {summary['total_questions']} questions")
        except Exception as e:
            print(f"‚ö†Ô∏è  Optimized evaluation had issues (expected): {e}")
        
        # Test optimization stats
        try:
            stats = suite.get_optimization_stats()
            print(f"‚úÖ Optimization stats: {len(stats)} categories")
        except Exception as e:
            print(f"‚ö†Ô∏è  Optimization stats unavailable: {e}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Evaluation suite testing failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_validation():
    """Test input validation."""
    print("\nüß™ Testing validation...")
    
    try:
        from src.agi_eval_sandbox.core.validation import InputValidator
        
        # Test valid inputs
        InputValidator.validate_model_name("gpt-4")
        InputValidator.validate_provider("openai")
        InputValidator.validate_temperature(0.5)
        InputValidator.validate_max_tokens(1000)
        print("‚úÖ Valid input validation passed")
        
        # Test invalid inputs (should raise exceptions)
        invalid_tests = [
            (lambda: InputValidator.validate_temperature(5.0), "temperature out of range"),
            (lambda: InputValidator.validate_max_tokens(-1), "negative max_tokens"),
            (lambda: InputValidator.validate_provider("invalid"), "invalid provider"),
        ]
        
        for test_func, test_name in invalid_tests:
            try:
                test_func()
                print(f"‚ùå {test_name} should have failed but didn't")
                return False
            except:
                print(f"‚úÖ {test_name} correctly rejected")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Validation testing failed: {e}")
        return False

async def test_health_monitoring():
    """Test health monitoring system."""
    print("\nüß™ Testing health monitoring...")
    
    try:
        from src.agi_eval_sandbox.core.health import health_monitor
        
        # Run health checks
        checks = await health_monitor.run_all_checks()
        print(f"‚úÖ Health checks completed: {len(checks)} checks")
        
        # Get overall health
        overall_status = health_monitor.get_overall_health()
        print(f"‚úÖ Overall health status: {overall_status.value}")
        
        # Get health summary
        summary = health_monitor.get_health_summary()
        print(f"‚úÖ Health summary generated: {len(summary['checks'])} checks")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Health monitoring failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_results_export():
    """Test results export functionality."""
    print("\nüß™ Testing results export...")
    
    try:
        from src.agi_eval_sandbox.core.results import Results, BenchmarkResult, EvaluationResult
        from src.agi_eval_sandbox.core.benchmarks import Score
        from datetime import datetime
        
        # Create mock results
        results = Results()
        
        # Create a mock benchmark result
        eval_results = [
            EvaluationResult(
                question_id="test_1",
                question_prompt="Test question 1",
                model_response="Test response 1",
                score=Score(value=0.8, passed=True),
                benchmark_name="test_benchmark",
                category="test"
            ),
            EvaluationResult(
                question_id="test_2", 
                question_prompt="Test question 2",
                model_response="Test response 2",
                score=Score(value=0.6, passed=True),
                benchmark_name="test_benchmark",
                category="test"
            )
        ]
        
        benchmark_result = BenchmarkResult(
            benchmark_name="test_benchmark",
            model_name="test_model",
            model_provider="mock",
            results=eval_results
        )
        
        results.add_benchmark_result(benchmark_result)
        
        # Test summary generation
        summary = results.summary()
        print(f"‚úÖ Results summary: {summary['total_questions']} questions, {summary['overall_score']:.2f} score")
        
        # Test JSON export
        results.export("json", "/tmp/test_results.json")
        print("‚úÖ JSON export completed")
        
        # Test dashboard display
        results.to_dashboard()
        print("‚úÖ Dashboard display completed")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Results export failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def run_all_tests():
    """Run all tests and return overall result."""
    print("üöÄ Starting comprehensive test suite for AGI Evaluation Sandbox")
    print("=" * 70)
    
    tests = [
        ("Imports", test_imports),
        ("Model Creation", test_model_creation),
        ("Benchmarks", test_benchmarks),
        ("Model Generation", test_model_generation),
        ("Evaluation Suite", test_evaluation_suite),
        ("Validation", test_validation),
        ("Health Monitoring", test_health_monitoring),
        ("Results Export", test_results_export),
    ]
    
    passed = 0
    failed = 0
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            if asyncio.iscoroutinefunction(test_func):
                result = await test_func()
            else:
                result = test_func()
            
            if result:
                passed += 1
                print(f"‚úÖ {test_name} PASSED")
            else:
                failed += 1
                print(f"‚ùå {test_name} FAILED")
                
        except Exception as e:
            failed += 1
            print(f"‚ùå {test_name} FAILED with exception: {e}")
    
    print("\n" + "=" * 70)
    print("üéØ TEST SUMMARY")
    print("=" * 70)
    print(f"‚úÖ Passed: {passed}")
    print(f"‚ùå Failed: {failed}")
    print(f"üìä Success Rate: {(passed/(passed+failed)*100):.1f}%")
    
    if failed == 0:
        print("\nüéâ ALL TESTS PASSED! The AGI Evaluation Sandbox is working correctly.")
        return True
    else:
        print(f"\n‚ö†Ô∏è  {failed} test(s) failed. Please review the errors above.")
        return False

if __name__ == "__main__":
    success = asyncio.run(run_all_tests())
    sys.exit(0 if success else 1)