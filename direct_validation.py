#!/usr/bin/env python3
"""
Direct Validation Test - Generation 1 Implementation

Simple direct test to validate core functionality.
"""

import sys
from pathlib import Path

# Add src to Python path
sys.path.insert(0, str(Path(__file__).parent / "src"))

print("ğŸš€ Starting Direct Validation Test")
print("Generation 1: Testing core functionality...")

try:
    # Test core imports
    print("\nğŸ“¦ Testing imports...")
    from agi_eval_sandbox.core.models import Model
    from agi_eval_sandbox.core.benchmarks import CustomBenchmark, Question, QuestionType, Score
    from agi_eval_sandbox.core.results import BenchmarkResult, EvaluationResult
    print("âœ… Core imports successful")
    
    # Test basic model creation
    print("\nğŸ¤– Testing model creation...")
    model = Model(provider="local", name="test_model")
    print(f"âœ… Model created: {model.name} ({model.provider})")
    
    # Test question creation
    print("\nâ“ Testing question creation...")
    question = Question(
        id="test_q1",
        prompt="What is 2+2?",
        correct_answer="4",
        question_type=QuestionType.SHORT_ANSWER
    )
    print(f"âœ… Question created: {question.id}")
    
    # Test benchmark creation
    print("\nğŸ—ï¸ Testing benchmark creation...")
    questions = [question]
    benchmark = CustomBenchmark(name="test_benchmark", questions=questions)
    print(f"âœ… Benchmark created: {benchmark.name}")
    
    # Test score creation
    print("\nğŸ“Š Testing score creation...")
    score = Score(value=0.85, passed=True, explanation="Good response")
    print(f"âœ… Score created: {score.value}")
    
    # Test evaluation result creation
    print("\nğŸ“‹ Testing evaluation result creation...")
    eval_result = EvaluationResult(
        question_id=question.id,
        question_prompt=question.prompt,
        model_response="4",
        score=score,
        benchmark_name=benchmark.name
    )
    print(f"âœ… Evaluation result created: {eval_result.question_id}")
    
    # Test benchmark result creation
    print("\nğŸ“ˆ Testing benchmark result creation...")
    benchmark_result = BenchmarkResult(
        benchmark_name=benchmark.name,
        model_name=model.name,
        model_provider=model.provider,
        results=[eval_result]
    )
    print(f"âœ… Benchmark result created: {benchmark_result.average_score:.3f}")
    
    print("\nğŸ‰ ALL TESTS PASSED!")
    print("âœ… Core research framework is working")
    print("ğŸš€ GENERATION 1 COMPLETE - Ready for Generation 2!")
    
except Exception as e:
    print(f"\nâŒ ERROR: {e}")
    import traceback
    traceback.print_exc()
    print("\nğŸ’¥ VALIDATION FAILED")
    sys.exit(1)

print("\nğŸ Direct validation completed successfully!")
